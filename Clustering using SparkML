#Installing Required Libraries
!pip install pyspark==3.1.2 -q
!pip install findspark -q

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

# FindSpark simplifies the process of using Apache Spark with Python

import findspark
findspark.init()

#import functions/Classes for sparkml

from pyspark.ml.clustering import KMeans
from pyspark.ml.feature import VectorAssembler

from pyspark.sql import SparkSession

#Create a spark session
#Create SparkSession
#Ignore any warnings by SparkSession command

spark = SparkSession.builder.appName("Clustering using SparkML").getOrCreate()

#Load the data in a csv file into a dataframe
!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/customers.csv

#Load the dataset into the spark dataframe
# using the spark.read.csv function we load the data into a dataframe.
# the header = True mentions that there is a header row in out csv file
# the inferSchema = True, tells spark to automatically find out the data types of the columns.

# Load customers dataset
customer_data = spark.read.csv("customers.csv", header=True, inferSchema=True)

#Print the schema of the dataset
# Each row in this dataset is about a customer. The columns indicate the orders placed
# by a customer for Fresh_food, Milk, Grocery and Frozen_Food
customer_data.printSchema()

#Create a feature vector
# Assemble the features into a single vector column
feature_cols = ['Fresh_Food', 'Milk', 'Grocery', 'Frozen_Food']
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
customer_transformed_data = assembler.transform(customer_data)

#must tell the KMeans algorithm how many clusters to create out of your data
number_of_clusters = 3

#Create a clustering model
#Create a KMeans clustering model
kmeans = KMeans(k = number_of_clusters)

#Train/Fit the model on the dataset
model = kmeans.fit(customer_transformed_data)

#Print Cluster Details
#The model is now trained. Time to evaluate the model.
# Make predictions on the dataset
predictions = model.transform(customer_transformed_data)

# Display the results
predictions.show(5)

#Display how many customers are there in each cluster.
predictions.groupBy('prediction').count().show()
#stop spark session
spark.stop()


